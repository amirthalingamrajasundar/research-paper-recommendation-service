{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5EecDZj5ARA4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "# Cell 1 (Modified): Install and Configure API\n",
    "!pip install -q -U google-genai pandas  # <-- Added -U to upgrade\n",
    "\n",
    "from google import genai\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from google.colab import userdata\n",
    "from tqdm.auto import tqdm # For a nice progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFOl9XQ2mkhU"
   },
   "outputs": [],
   "source": [
    "# --- Configure the API ---\n",
    "try:\n",
    "    # Get the key from Colab secrets\n",
    "    GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print(\"Gemini API configured successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error: Could not configure Gemini API. Make sure 'GEMINI_API_KEY' is set in Colab secrets.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHUHJ_cJOUzi"
   },
   "outputs": [],
   "source": [
    "# all_papers = pd.read_json(\"/content/drive/MyDrive/google_colab/arxiv/sample-data-5000.json\", lines=True)\n",
    "all_papers = pd.read_json(\"/content/drive/MyDrive/google_colab/arxiv/arxiv-metadata-oai-snapshot.json\", lines=True)\n",
    "all_papers['new_id'] = all_papers['id'].astype(str).str.replace('.','').astype(int)\n",
    "\n",
    "all_papers = all_papers.rename(columns={'id': 'arxiv_id'})\n",
    "all_papers = all_papers.rename(columns={'new_id': 'id'})\n",
    "\n",
    "all_papers.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GypC34cIOgB"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Define the Async \"Teacher\" Function\n",
    "\n",
    "async def get_similarity_score_async(paper1, paper2):\n",
    "    \"\"\"\n",
    "    Asks Gemini for a similarity score between two papers, asynchronously.\n",
    "    Returns a float score or an Exception on failure.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a research assistant. On a scale of 0.0 to 1.0, how relevant is Paper B as a recommendation for someone who just finished reading Paper A?\n",
    "    0.8 <=recommendation<= 1.0 is best recommendation\n",
    "    0.5 <=recommendation < 0.8 is medium recommendation\n",
    "    0.0 <= recommendation < 0.5 is negative recommendation\n",
    "    Do not explain your reasoning. Respond ONLY with the numerical score (e.g., 0.85).\n",
    "\n",
    "    Paper A:\n",
    "    Title: {paper1['title']}\n",
    "    Submitter: {paper1['submitter']}\n",
    "    Authors: {paper1['authors']}\n",
    "    Categories: {paper1['categories']}\n",
    "    Abstract: {paper1['abstract']}\n",
    "\n",
    "    Paper B:\n",
    "    Title: {paper2['title']}\n",
    "    Submitter: {paper2['submitter']}\n",
    "    Authors: {paper2['authors']}\n",
    "    Categories: {paper2['categories']}\n",
    "    Abstract: {paper2['abstract']}\n",
    "\n",
    "    Score:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(paper1['title'])\n",
    "        print(paper2['title'])\n",
    "        # Call the API using the correct async method\n",
    "        response = await client.aio.models.generate_content(\n",
    "            model='gemini-2.5-flash',\n",
    "            contents=prompt\n",
    "        )\n",
    "\n",
    "        # Safely parse the score\n",
    "        score_text = response.text.strip().replace(\",\", \".\")\n",
    "        score = float(score_text)\n",
    "\n",
    "        # Clamp score to be safe\n",
    "        return max(0.0, min(1.0, score))\n",
    "\n",
    "    except Exception as e:\n",
    "        # Instead of just printing, return the exception\n",
    "        # This lets our main loop handle it gracefully\n",
    "        return e\n",
    "    finally:\n",
    "        # A small non-blocking sleep to respect rate limits\n",
    "        await asyncio.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zP_mit2WDq-3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# The set to track pairs we've already processed\n",
    "# We store a sorted tuple (id1, id2) so (A,B) is the same as (B,A)\n",
    "seen_pairs = set()\n",
    "NUM_OF_PAIRS = 50000\n",
    "\n",
    "while len(seen_pairs) < NUM_OF_PAIRS:\n",
    "  p1i = random.choice(all_papers['id'])\n",
    "  p2i = random.choice(all_papers['id'])\n",
    "  if p1i == p2i:\n",
    "    continue\n",
    "\n",
    "  pair_key = tuple(sorted([p1i, p2i]))\n",
    "  if pair_key not in seen_pairs:\n",
    "    seen_pairs.add(pair_key)\n",
    "    print(len(seen_pairs))\n",
    "  print(len(seen_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vhWfVOeO9JF"
   },
   "outputs": [],
   "source": [
    "all_papers[all_papers['id'] == 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyVI5qQFLAAh"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Run in Parallel (Batches of 10)\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "labeled_pairs = []\n",
    "pairs_to_process = list(seen_pairs) # Convert set to a list to be batched\n",
    "\n",
    "# Counters for your buckets\n",
    "positive = 0\n",
    "medium = 0\n",
    "negative = 0\n",
    "\n",
    "print(f\"Starting parallel processing for {len(pairs_to_process)} pairs in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "async def main():\n",
    "    # Grant access to the global counters\n",
    "    global positive, medium, negative\n",
    "\n",
    "    # Use tqdm for a progress bar\n",
    "    for i in tqdm(range(0, len(pairs_to_process), BATCH_SIZE)):\n",
    "\n",
    "        batch_keys = pairs_to_process[i:i + BATCH_SIZE]\n",
    "        tasks = []\n",
    "\n",
    "        for p1i, p2i in batch_keys:\n",
    "            # Use the FAST dictionary lookup\n",
    "            p1 = all_papers[all_papers['id'] == p1i]\n",
    "            p2 = all_papers[all_papers['id'] == p2i]\n",
    "            tasks.append(get_similarity_score_async(p1, p2))\n",
    "\n",
    "        # Run the batch of tasks in parallel\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # Process results from the batch\n",
    "        for (p1i, p2i), score in zip(batch_keys, results):\n",
    "\n",
    "            # Check if the result was a valid score (not an exception)\n",
    "            if isinstance(score, float):\n",
    "                print(f\"score is {score}\")\n",
    "                p1 = all_papers[all_papers['id'] == p1i]\n",
    "                p2 = all_papers[all_papers['id'] == p2i]\n",
    "\n",
    "                # Classify the score into buckets\n",
    "                pair_type = \"\"\n",
    "                if score >= 0.8:\n",
    "                    pair_type = \"positive\"\n",
    "                    positive += 1\n",
    "                elif score >= 0.5:\n",
    "                    pair_type = \"medium\"\n",
    "                    medium += 1\n",
    "                elif score >= 0:\n",
    "                    pair_type = \"negative\"\n",
    "                    negative += 1\n",
    "\n",
    "                # --- FIX THE TYPEERROR ---\n",
    "                # Keys must be strings (e.g., \"text_1\"), not dictionary objects\n",
    "                labeled_pairs.append({\n",
    "                    \"p1_id\": int(p1[\"id\"]),\n",
    "                    \"p2_id\": int(p2[\"id\"]),\n",
    "                    \"score\": score,\n",
    "                    \"pair_type\": pair_type\n",
    "                })\n",
    "            elif isinstance(score, Exception):\n",
    "                print(f\"  [Error processing pair ({p1i}, {p2i})]: {score}\")\n",
    "\n",
    "# --- Run the main async function ---\n",
    "await main()\n",
    "\n",
    "print(f\"\\nProcessing complete.\")\n",
    "print(f\"Successfully generated {len(labeled_pairs)} labels.\")\n",
    "print(f\"Buckets: {positive} Positive, {medium} Medium, {negative} Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAg9kH59m1nG"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Review and Save\n",
    "if labeled_pairs:\n",
    "    df = pd.DataFrame(labeled_pairs)\n",
    "\n",
    "    print(\"\\n--- Pairs per Type ---\")\n",
    "    print(df['pair_type'].value_counts())\n",
    "\n",
    "    # Save to a file for your fine-tuning notebook\n",
    "    df.to_csv(\"my_labeled_training_data.csv\", index=False)\n",
    "\n",
    "    print(\"\\nSuccessfully saved to 'my_labeled_training_data.csv'\")\n",
    "\n",
    "else:\n",
    "    print(\"No labeled pairs were generated. Try increasing MAX_ATTEMPTS or check your sampling logic.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Research paper recommendation",
   "language": "python",
   "name": "recomm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
